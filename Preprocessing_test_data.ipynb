{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocessing_test_data.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXsw2K1M5D-S"
      },
      "source": [
        "import pandas as pd \n",
        "import os\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk import Tree\n",
        "from nltk.draw.tree import TreeView\n",
        "import os\n",
        "from spacy import displacy\n",
        "import spacy\n",
        "from collections import Counter\n",
        "import en_core_web_sm\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nlp = en_core_web_sm.load()\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0W99Oio65tr"
      },
      "source": [
        "!pip install benepar\n",
        "!pip install cython numpy\n",
        "!pip install Keras\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEh_8lvc6851",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e11b3de8-435c-4796-e36c-69f2333ea51f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t49i1E2o7oD2"
      },
      "source": [
        "STIAHNUTIE TWITTER DAT\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvaknuZN7nLl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07d02226-e894-4fb3-c8d4-070a9db076d4"
      },
      "source": [
        "import tweepy as tw\n",
        "api_key= 'Xn4lLpcmofHaZBd7NvgyNTSaE'\n",
        "api_secret= 'pPnlRPfbSA1rwgNM7L6NL1GIyOaruWHYRpaK4v9GmnomMtAaLp'\n",
        "access_token= '1387459078046298115-1n3EyAk1rNf8mCfkubpdIvQ3FusIuh'\n",
        "access_token_secret= 'TiMz3h1SN30lyGxEqeHozrSli1Q7WDi2mWwD5CFTNb8y6'\n",
        "\n",
        "auth = tw.OAuthHandler(api_key, api_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tw.API(auth, wait_on_rate_limit=True)\n",
        "words2search = \"#economic -filter:retweets\" #+ coronavirus\n",
        "date = \"2019-11-16\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "@ArnoSnyman4 @_TshegoM Don’t drive BMW and manage my finances well am Black n proud.Tried to buy a car Absa decline… https://t.co/BKjiUGLlpF\n",
            "RT @ajaymaken: My tributes to Dr A.K.Walia at the prayer meet-Underlined Dr Walia's contributions to Delhi as its Finance,Health &amp; Urban De…\n",
            "@ArnoSnyman4 @_TshegoM Don’t drive BMW and manage my finances well am Black n proud.Tried to buy a car Absa decline… https://t.co/BKjiUGLlpF\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPhqrrkDeJnw"
      },
      "source": [
        "import csv\n",
        "import time\n",
        "import hashlib\n",
        "\n",
        "last_id = None\n",
        "result = True\n",
        "all_tweets = []\n",
        "all_hashes = []\n",
        "\n",
        "num_of_requests = 0\n",
        "\n",
        "while result:\n",
        "  result = api.search(q=words2search, count=500, tweet_mode='extended', max_id=last_id)\n",
        "  \n",
        "  with open('new_tweets.txt', 'a') as f:\n",
        "    for res in result:\n",
        "      hash = hashlib.md5(res.full_text.encode('utf-8'))\n",
        "      if hash in all_hashes:\n",
        "        continue\n",
        "      all_hashes.append(hash)\n",
        "      all_tweets.append(res.full_text)\n",
        "      f.write(res.full_text)\n",
        "      f.write('\\n---TWEET-BREAK---\\n')\n",
        "\n",
        "  f.close()\n",
        "\n",
        "  num_of_requests += 1\n",
        "  last_id = result[-1]._json['id'] - 1\n",
        "  print('Iteration: {it} Finished iteration with last_id: {i} with total tweets of: {total}'.format(it=num_of_requests, i=last_id, total=len(all_tweets)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm44j-Z45_-l"
      },
      "source": [
        "import re\n",
        "def remove_url(txt):\n",
        "  txt = txt.replace('\\n', ' ').replace('\\r', ' ')\n",
        "  return \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", txt).split())\n",
        "\n",
        "def remove_numbers(txt):\n",
        "  pattern = r'[0-9]'\n",
        "  # Match all digits in the string and replace them by empty string\n",
        "  return re.sub(pattern, '', txt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tKxRKxwxe2S"
      },
      "source": [
        "with open('new_tweets.txt', 'r') as f:\n",
        "  content = f.read()\n",
        "  content_list = content.split(\"\\n---TWEET-BREAK---\\n\")\n",
        "  data = pd.DataFrame(content_list,columns =['Tweets'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ctIO-gE52cu"
      },
      "source": [
        "data['Tweets'] = data['Tweets'].apply(lambda row: remove_url(row))\n",
        "data['Tweets'] = data['Tweets'].apply(lambda row: remove_numbers(row))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIbIsKXrIJhP"
      },
      "source": [
        "data = data.drop_duplicates(subset=['Tweets'])\n",
        "data.replace(\"\", float(\"NaN\"), inplace=True)\n",
        "data.dropna(subset = [\"Tweets\"],inplace=True)\n",
        "data.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szuJuBmvCaXd"
      },
      "source": [
        "data.to_csv('tweetsdataset.csv')\n",
        "!cp tweetsdataset.csv \"/content/drive/My Drive/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPSeKaMRyMxU"
      },
      "source": [
        "data = pd.read_csv('/content/drive/My Drive/tweetsdataset.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbU9XSTx6-bs"
      },
      "source": [
        "class Sentence:\n",
        "  def __init__(self):\n",
        "    self.words = []\n",
        "    self.wholeSentence = None\n",
        "    self.tree = None\n",
        "    self.NerTag = None\n",
        "\n",
        "  def appendWords(self, word): \n",
        "    self.words.append(word) \n",
        "\n",
        "class Word:\n",
        "  def __init__(self):\n",
        "    self.word = None\n",
        "    self.lemma = None\n",
        "    self.synsetName = None\n",
        "    self.synsetDefinition = None\n",
        "    self.posTag = None\n",
        "    self.grammarTag = None\n",
        "    self.synonyma = []\n",
        "    self.antonyma = None\n",
        "    self.negation = None\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SI0xVrwG8ge",
        "outputId": "f7f4fcf1-f925-4027-da62-0c8eaad2dc34"
      },
      "source": [
        "\n",
        "from nltk.corpus import stopwords \n",
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqa8ZLO37Abn"
      },
      "source": [
        "\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def get_first_sense(word, pos=None):\n",
        "    if pos:\n",
        "        synsets = wordnet.synsets(word,pos)\n",
        "    else:\n",
        "        synsets = wordnet.synsets(word)\n",
        "    if not synsets:\n",
        "      return None\n",
        "    else:\n",
        "      return synsets[0]\n",
        "\n",
        "data['Postag-Lemmatized Sentence'] = None\n",
        "data['Pos-Lem Sen without tags'] = None\n",
        "listofSentences = []\n",
        "for index,row in data.iterrows():\n",
        "  #inicializácia objektov\n",
        "  lm = WordNetLemmatizer()\n",
        "  sentenceObject = Sentence()\n",
        "\n",
        "  newEditedSentence = \"\"\n",
        "  taggedlematizedsentence = []\n",
        "\n",
        "  stop_words = stopwords.words('english')\n",
        "\n",
        "  sentence = row['Tweets']\n",
        "  print(index)\n",
        "  tokenized_sentence = text_to_word_sequence(sentence)\n",
        "  filtered_sentence = [w for w in tokenized_sentence if not w in stop_words] \n",
        "  tagged_sentence = nltk.pos_tag(filtered_sentence)\n",
        "\n",
        "  for token in tagged_sentence:\n",
        "    wordObject = Word()\n",
        "    best_synset = None\n",
        "    postag = get_wordnet_pos(token[1])\n",
        "    wordObject.word = token[0]\n",
        "    if postag is None or postag is '': # not supply tag in case of None\n",
        "      lemma = lm.lemmatize(token[0])\n",
        "      taggedlematizedsentence.append((lemma,token[1]))\n",
        "      newEditedSentence+=lemma + \" \"\n",
        "      best_synset = get_first_sense(lemma)\n",
        "      wordObject.lemma = lemma\n",
        "      wordObject.posTag = token[1]\n",
        "    else:\n",
        "      lemma = lm.lemmatize(token[0], postag) \n",
        "      taggedlematizedsentence.append((lemma,token[1])) # vráti sa mi lemma a pridam pôvodný POS-tag, nie ten ktorý mám z Wordnetu \n",
        "      best_synset = get_first_sense(lemma,postag)\n",
        "\n",
        "      newEditedSentence+=lemma + \" \"\n",
        "      wordObject.lemma = lemma\n",
        "      wordObject.posTag = token[1]\n",
        "\n",
        "    if best_synset is None:\n",
        "      #print(\"Slovo nema viac významov\") \n",
        "      wordObject.synsetName = None\n",
        "      wordObject.synsetDefinition = None\n",
        "      #TODO negation handling   \n",
        "    else:\n",
        "      wordObject.synsetName = best_synset.name()\n",
        "      wordObject.synsetDefinition = best_synset.definition()\n",
        "      for synlemma in best_synset.lemmas():\n",
        "        wordObject.synonyma.append(synlemma.name())\n",
        "        if synlemma.antonyms():\n",
        "          wordObject.antonyma = synlemma.antonyms()[0].name()\n",
        "          #print(wordObject.lemma,synlemma.antonyms()[0].name())\n",
        "\n",
        "\n",
        "    sentenceObject.appendWords(wordObject)\n",
        "  sentenceObject.wholeSentence = newEditedSentence\n",
        "  data.at[index,'Postag-Lemmatized Sentence'] = taggedlematizedsentence\n",
        "  data.at[index,'Pos-Lem Sen without tags'] = newEditedSentence\n",
        "  data.at[index,'Sentence-word object'] = sentenceObject\n",
        "  listofSentences.append(sentenceObject)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-nFLHpD7MiL"
      },
      "source": [
        "import pickle \n",
        "filehandler = open(\"/content/drive/My Drive/Test_Twitter_Sentences\", 'wb') \n",
        "pickle.dump(listofSentences, filehandler, pickle.HIGHEST_PROTOCOL)\n",
        "filehandler.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}